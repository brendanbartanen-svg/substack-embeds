<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>How Brendan Reviews: A Rubric and Workflow for Manuscript Review</title>
<style>
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }
  body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    line-height: 1.75;
    color: #1a1a1a;
    font-size: 16px;
    background: #ffffff;
    padding: 28px 32px 40px;
  }

  /* ---- Typography ---- */
  h1 {
    font-size: 1.6em;
    font-weight: 700;
    margin-bottom: 6px;
    color: #111;
    line-height: 1.3;
  }
  h2 {
    font-size: 1.25em;
    font-weight: 700;
    margin-top: 1.8em;
    margin-bottom: 0.5em;
    color: #222;
    padding-bottom: 4px;
    border-bottom: 1px solid #e5e5e5;
  }
  h3 {
    font-size: 1.05em;
    font-weight: 600;
    margin-top: 1.3em;
    margin-bottom: 0.4em;
    color: #333;
  }
  p {
    margin-bottom: 0.8em;
  }

  /* ---- Blockquotes ---- */
  blockquote {
    border-left: 3px solid #c0c0c0;
    padding: 8px 16px;
    margin: 0.8em 0;
    color: #555;
    font-style: italic;
    background: #fafafa;
    border-radius: 0 4px 4px 0;
  }

  /* ---- Lists ---- */
  ul, ol {
    padding-left: 1.4em;
    margin-bottom: 0.8em;
  }
  li {
    margin-bottom: 0.35em;
  }

  /* ---- Horizontal rules ---- */
  hr {
    border: none;
    border-top: 1px solid #ddd;
    margin: 1.8em 0;
  }

  /* ---- Inline code ---- */
  code {
    background: #f0f0f0;
    padding: 2px 5px;
    border-radius: 3px;
    font-size: 0.88em;
  }

  /* ---- Emphasis ---- */
  strong {
    font-weight: 600;
  }

  /* ---- Subtitle / first em block ---- */
  h1 + p > em {
    color: #666;
    font-size: 0.88em;
    display: block;
    margin-bottom: 0.5em;
  }
</style>
</head>
<body>
<h1>How Brendan Reviews: A Rubric and Workflow for Manuscript Review</h1>
<p><em>Synthesized from 70+ peer reviews across AERA, AERJ, EEPA, EdR, JPAM, JOLE, EFP, EAQ, RER, JTE, COEP, EMEC, SSR, ILR, JPE, AJE, and other education research journals (2021–2025).</em></p>
<hr />
<h2>1. Review Structure and Organization</h2>
<p>Every review follows a consistent architecture:</p>
<ol>
<li>
<p><strong>Opening paragraph(s):</strong> Factual summary of the paper — what data are used, what question is asked, what methods are employed, and what the main findings are. This comes <em>before</em> any evaluative language.</p>
</li>
<li>
<p><strong>Overall assessment statement:</strong> A clear signal of how Brendan sees the paper. This ranges from enthusiastic ("I am a big fan of this paper") to direct ("the current form falls short"). This is typically signaled in the opening 1–2 paragraphs but may be implicit in the opening framing and made explicit as major issues emerge.</p>
</li>
<li>
<p><strong>Major comments:</strong> The core of the review. Organized thematically (not by page number), typically focusing on 3–6 substantive issues. Each major comment is a developed paragraph or set of paragraphs, not a bullet point. Major comments involve threats to validity, conceptual confusion, or issues that prevent the paper from answering its stated research questions.</p>
</li>
<li>
<p><strong>Minor comments:</strong> Shorter observations about writing, presentation, missing citations, figure/table improvements, and typos. Sometimes presented as a numbered list, sometimes as flowing paragraphs — the format depends on density and the overall tone of the review.</p>
</li>
<li>
<p><strong>Closing:</strong> Varies by recommendation — may express a path forward for revision, defer to the editor, or summarize the key issue that needs resolution.</p>
</li>
</ol>
<hr />
<h2>2. Opening Style</h2>
<p>The opening serves two functions: (a) demonstrate to the editor that the paper was read carefully, and (b) establish context before evaluation.</p>
<p><strong>Template pattern:</strong></p>
<blockquote>
<p>The authors use [data source] to examine [research question]. [1-2 sentences on method and key findings]. [Overall assessment — positive framing of the question's importance, followed by the reviewer's position on execution].</p>
</blockquote>
<p><strong>Examples of the spectrum:</strong></p>
<ul>
<li><strong>Strong paper:</strong> "I am a big fan of this paper. I think it's insightful and relevant for several literatures... The analysis seems well-implemented and thorough, and the paper itself is well-motivated, grounded in theory, and clear."</li>
<li><strong>Mixed paper:</strong> "There is some useful information here, particularly in showing [X]... However, [the core analytical approach] feels a bit out of place."</li>
<li><strong>Weak paper:</strong> "The question is worthy of attention, generally, but the execution of the analysis does not produce credible estimates of [the parameter of interest]."</li>
<li><strong>Paper with fundamental issues:</strong> "Unfortunately, there are a number of important conceptual and interpretation issues that inhibit the paper's ability to successfully contribute."</li>
</ul>
<p><strong>Key principle:</strong> Always acknowledge the importance or worthiness of the research question, even in negative reviews. Critique is directed at execution, not at the authors' intellect or ambition.</p>
<hr />
<h2>3. Tone and Voice</h2>
<h3>Core principles:</h3>
<ul>
<li><strong>Direct but collegial.</strong> Never dismissive of authors; never hedging so much that the critique loses force.</li>
<li><strong>Intellectually honest.</strong> Acknowledge uncertainty ("I'm not sure..."), limits of expertise ("I am not a critical scholar"), and the difficulty of the work.</li>
<li><strong>Constructive.</strong> Frame problems as addressable when they are. When they aren't, be straightforward about that too.</li>
<li><strong>Proportionate.</strong> Praise what genuinely works; don't manufacture positives to soften a negative review.</li>
<li><strong>Socratic by default.</strong> Rather than declaring errors, pose questions that guide the authors to see the problem themselves. This isn't an occasional rhetorical strategy — it's a baseline approach. "What is the base rate here?" "What does it mean that [X]?" "Can the authors say a bit more about..." This is more effective than assertion because it invites the authors to engage with the concern rather than become defensive.</li>
</ul>
<h3>Characteristic language for softened critique:</h3>
<ul>
<li>"I wonder if..." / "I wonder whether..."</li>
<li>"I'm not sure that..."</li>
<li>"It's not immediately clear..."</li>
<li>"I'm curious about..."</li>
<li>"Can you say more about..."</li>
<li>"I'd encourage the authors to..."</li>
</ul>
<h3>Characteristic language for direct critique:</h3>
<ul>
<li>"I have pretty serious misgivings about..."</li>
<li>"I do not think the current manuscript produces a credible answer"</li>
<li>"I remain skeptical that..." / "I remain highly skeptical..."</li>
<li>"This doesn't seem quite right..."</li>
<li>"I'm having a hard time believing..."</li>
<li>"The empirical warrant is pretty weak"</li>
<li>"I don't find this justification compelling"</li>
<li>"There is almost no discussion of..."</li>
</ul>
<h3>Characteristic language for praise:</h3>
<ul>
<li>"I appreciate that..."</li>
<li>"I commend the authors for..."</li>
<li>"This is a useful exercise..."</li>
<li>"The paper is well-written and the front half, in particular, is quite strong"</li>
</ul>
<h3>Phrases to use frequently:</h3>
<ul>
<li>"To me, ..." (appears in nearly every review — signals a subjective assessment while remaining authoritative)</li>
<li>"As a related point, ..." (connects concerns to show they're part of a bigger issue)</li>
<li>"More generally, ..." (transitions from a specific critique to a broader pattern)</li>
<li>"Putting aside [X], ..." (acknowledges complexity before focusing on a specific issue)</li>
</ul>
<hr />
<h2>4. Substantive Review Dimensions</h2>
<h3>4A. Causal Identification and Research Design</h3>
<p>This is the single most important dimension. For any paper making causal or quasi-causal claims, the review must address:</p>
<ul>
<li><strong>What is the identification strategy?</strong> Is it stated explicitly? If not, what does the design implicitly assume?</li>
<li><strong>What assumptions are required?</strong> Are they articulated? Are they defensible? What evidence supports them?</li>
<li><strong>What are the most salient threats to validity?</strong> Omitted variable bias, reverse causality, selection, measurement error?</li>
<li><strong>Is there a mismatch between the design and the causal claim?</strong> Papers that use descriptive methods but interpret results causally are a frequent problem.</li>
<li><strong>For difference-in-differences:</strong> Are parallel trends shown? Is the treatment timing exogenous? Are there pre-trends in event-study figures?</li>
<li><strong>For fixed effects models:</strong> Is there an age-period-cohort problem? Is the identifying variation clear and defensible?</li>
<li><strong>For instrumental variables:</strong> Is the instrument strong? Is the exclusion restriction plausible?</li>
</ul>
<p><strong>Recurring critique pattern:</strong> "The overall point here is that there needs to be much more precise articulation of why the approach you're taking is suitable for this research question, what the identification assumptions are, and what the most salient threats to validity might be."</p>
<h3>4B. Endogeneity and Mechanical Relationships</h3>
<p>A signature concern. Be vigilant for:</p>
<ul>
<li><strong>Variables that appear on both sides of the equation.</strong> Example: "% of Black students is on both sides of the equation, which creates a huge endogeneity problem. The discipline index is the ratio of offenses to number of students by race, so you already have, effectively, % of Black students in the model."</li>
<li><strong>Mechanical correlations in value-added.</strong> Example: "When you form the VA estimates via equations 1 and 2, the student-level error remains in the residual... These same factors likely impact the longer-run outcomes as well, so they appear on both the right- and left-hand-sides of equation 3, leading to a mechanical positive correlation."</li>
<li><strong>Control variables that are endogenous.</strong> Example: "% of novice teachers is also endogenous to a rolling average of teacher turnover (novice teachers must be new-to-school and vacancies arise because teachers left in prior years)."</li>
</ul>
<h3>4C. Value-Added Models and Effect Attribution</h3>
<p>When papers use value-added methods (for teachers, principals, coaches, or other units):</p>
<ul>
<li><strong>Is the attribution reasonable?</strong> Value-added is a variance decomposition exercise — the credibility rests on whether it's reasonable to attribute residual variation to the unit in question.</li>
<li><strong>Cross-year stability:</strong> Single-year VA estimates are dominated by transient noise. "It's reached a point where it's become indefensible to interpret the between-teacher SD from single-year VA estimates as a reasonable estimate of the true SD of teacher effects."</li>
<li><strong>Jackknife/leave-out estimation:</strong> Are mechanical correlations addressed appropriately?</li>
<li><strong>What outcome is being used?</strong> Is it appropriate for the unit being evaluated?</li>
</ul>
<h3>4D. Sample Selection and Composition</h3>
<ul>
<li><strong>Who is in the sample and who is excluded?</strong> Are exclusions justified and transparent?</li>
<li><strong>Is there selection into treatment?</strong> "Choosing to switch to a residency model is not a random event."</li>
<li><strong>Response rates for surveys:</strong> Low response rates are flagged consistently. "The biggest limitation of this study is the low response rate (roughly 7%)."</li>
<li><strong>Does the sample composition affect generalizability?</strong> Note when findings apply only to a specific context.</li>
</ul>
<h3>4E. Measurement and Construct Validity</h3>
<ul>
<li><strong>Does the measure capture the construct?</strong> "The notion of using community SES as a proxy for the 'managerial experience of potential LSC candidates' requires quite a stretch."</li>
<li><strong>Observation scores and subjective measures:</strong> Be skeptical. These reflect rater characteristics, school context, and measurement error in addition to (or instead of) the construct of interest.</li>
<li><strong>Self-reported data:</strong> Note limitations, especially for behavioral or attitudinal measures.</li>
<li><strong>Proxy variables:</strong> When a proxy is used, assess how well it maps to the underlying concept.</li>
<li><strong>Algorithmic/computational methods:</strong> When text analysis (e.g., LIWC), topic modeling, or other computational approaches are used, assess whether the method itself is valid for the purpose. Example: dictionary-based text analysis "does not consider the context in which the word appears," which may invalidate the measurement approach entirely.</li>
<li><strong>Data provenance:</strong> For variables constructed from administrative data, is the construction transparent? Are researcher-generated measures clearly documented? Gaps here can hide measurement problems.</li>
<li><strong>Plausibility checks:</strong> Do reported descriptive statistics align with institutional constraints or prior knowledge? Implausibly high or low percentages that violate known requirements suggest data quality issues that need investigation.</li>
</ul>
<h3>4F. Model Specification and Functional Form</h3>
<ul>
<li><strong>Are functional form choices justified?</strong> Example: Interacting a quartic for teacher experience with a linear principal characteristic "makes a strict functional form restriction that the impact on teacher RTE is uniform at all levels of teacher experience."</li>
<li><strong>Control variable logic:</strong> "You should clearly articulate why a particular control variable is included. What specific source of bias is it attempting to address?"</li>
<li><strong>Fixed effects combinations:</strong> Be alert to collinearity and the age-period-cohort problem when combining unit FE + time FE + experience controls.</li>
<li><strong>Standard errors:</strong> How are they clustered? Is the clustering appropriate for the design? "Clustered by school-grade-year feels arbitrary and perhaps too narrow."</li>
<li><strong>Model labeling and transparency:</strong> Are all estimated specifications clearly labeled and connected to the reported results? Unlabeled equations or ambiguity about which model produced which results is a red flag.</li>
</ul>
<h3>4G. Statistical Issues</h3>
<ul>
<li><strong>Multiple hypothesis testing:</strong> Are corrections applied when many comparisons are made?</li>
<li><strong>Statistical power:</strong> Is the sample large enough to detect the effects of interest? Are null results potentially underpowered?</li>
<li><strong>Sub-group comparisons:</strong> "If you're going to compare across sub-samples, you should report tests of statistical significance for the null hypothesis that the coefficient estimates are the same across the two models."</li>
<li><strong>Confidence intervals on figures:</strong> "Figure 2 should have confidence intervals."</li>
<li><strong>Denominator issues:</strong> When comparing rates across groups of different sizes, "the likelihood of extreme outcomes is larger" in smaller groups.</li>
<li><strong>Precision vs. power:</strong> When effect sizes are non-trivial but imprecisely estimated, is the interpretation of "no effect" justified, or is the analysis simply underpowered? "Many of the point estimates are positive and non-trivial in magnitude, but aren't statistically significant because of large standard errors."</li>
</ul>
<h3>4H. Conceptual Framework and Contribution</h3>
<ul>
<li><strong>Is the contribution clear relative to prior work?</strong> "The distribution of teacher quality is not a new question, so I'm really struggling with how to understand the contribution."</li>
<li><strong>Is the theoretical framework adequate?</strong> Does it motivate the specific empirical approach?</li>
<li><strong>Does the paper overreach in its claims?</strong> Flag when conclusions go beyond what the design supports. "Fundamentally descriptive" is a frequent characterization when causal language is inappropriate.</li>
<li><strong>Policy relevance:</strong> "What do we do with this information?" Connect methodological rigor to whether findings actually inform practice or policy.</li>
</ul>
<h3>4I. Generalizability and External Validity</h3>
<ul>
<li><strong>Context specificity:</strong> Are findings specific to a particular state, district, time period, or policy environment? Note when the paper doesn't adequately address how portable the results are.</li>
<li><strong>Sample representativeness:</strong> How do the studied units compare to the broader population of interest? "Can the authors say a bit more about the low participation rate... I did want to know more about the extent to which these principals may or may not be different from the typical principal."</li>
<li><strong>Ambition relative to data:</strong> Sometimes descriptive papers underexploit rich data. "My one piece of critical feedback would be that perhaps the paper is not ambitious enough given the data the authors have."</li>
</ul>
<h3>4J. Mechanism and Outcome Interpretation</h3>
<ul>
<li><strong>Administrative vs. real effects:</strong> When outcomes could reflect multiple mechanisms, is it clear what the paper is actually measuring? "The results could simply be an administrative response to the rating, rather than an actual change in student behavior."</li>
<li><strong>Mechanical vs. behavioral effects:</strong> In policy evaluations, distinguish between discontinuities that arise mechanically from policy rules vs. those reflecting actual behavioral or performance changes.</li>
<li><strong>Overreach in implications:</strong> A descriptive analysis may support many interpretations, but "this descriptive analysis is not enough to lead to the various implications" the authors propose. Implications should be bounded by the methodology.</li>
</ul>
<h3>4K. Writing and Accessibility</h3>
<p>This is a core value, not a minor issue. Clarity and accessibility are raised even in reviews of otherwise strong papers.</p>
<ul>
<li><strong>Jargon:</strong> "Can you rewrite this paragraph without jargon?"</li>
<li><strong>Inaccessibility:</strong> "This paper is very inaccessible. Even if this paper were submitted to a highly technical journal, I would raise this concern."</li>
<li><strong>Model presentation:</strong> Are equations clearly explained? Can a non-specialist follow the logic?</li>
<li><strong>Figures and tables:</strong> Are they self-contained with adequate notes? Is the visualization effective?</li>
</ul>
<hr />
<h2>5. How to Differentiate Major vs. Minor Issues</h2>
<h3>Major issues:</h3>
<ul>
<li>Threaten the validity of core findings</li>
<li>Involve conceptual confusion or misalignment between design and claims</li>
<li>Cannot be fixed with small changes (require fundamental rethinking)</li>
<li>Would change the reader's interpretation of the results</li>
<li><strong>Organizational coherence:</strong> When a paper pursues multiple methodological approaches or research questions without clear integration, this is a major issue. "The paper feels a bit disjointed... much of the paper is devoted to a conceptually different question."</li>
</ul>
<p><strong>Language markers:</strong> "serious misgivings," "deep reservations," "substantial shortcoming," "I do not think the current manuscript produces a credible answer"</p>
<h3>Minor issues:</h3>
<ul>
<li>Presentation, writing, and clarity improvements</li>
<li>Missing citations that don't undermine the argument</li>
<li>Typos and formatting</li>
<li>Suggestions for strengthening that aren't required for the core contribution</li>
<li>Extensions that would be nice but aren't necessary</li>
</ul>
<p><strong>Language markers:</strong> "Small things," "one small comment," "a couple of minor points," "I have a few comments and suggestions but identified no major issues"</p>
<hr />
<h2>6. Closing Style</h2>
<p>The closing should match the overall assessment:</p>
<ul>
<li><strong>Accept/minor revisions:</strong> Express confidence in the contribution, frame remaining issues as addressable. "I commend the authors for their work."</li>
<li><strong>Revise and resubmit:</strong> Identify a path forward while being explicit about what must change. "I do think there's a path forward for the paper if the authors are able to address the issues raised."</li>
<li><strong>Reject/major revision:</strong> Be direct without false hope. Do not soften with manufactured positives. "I cannot make a positive recommendation because of weaknesses in the analytical approach."</li>
<li><strong>Deference to editor:</strong> "Ultimately, of course, that is not my decision to make."</li>
</ul>
<hr />
<h2>7. Revision Reviews (R&amp;Rs)</h2>
<p>When reviewing a revised manuscript:</p>
<ul>
<li><strong>Acknowledge improvements explicitly.</strong> "I thank the authors for the further exploration of [X]. I think their responses and changes to the manuscript are reasonable."</li>
<li><strong>Be clear about what remains unresolved.</strong> "I am not completely satisfied with the authors' response here, though perhaps I did not articulate my concern clearly enough."</li>
<li><strong>Don't raise entirely new issues unless they are fundamental.</strong> Revision reviews should primarily assess whether the original concerns were addressed.</li>
<li><strong>Be willing to accept reasonable explanations</strong> even if they don't perfectly resolve the concern.</li>
<li><strong>Distinguish between concerns that can be addressed in the current paper vs. those that are limitations to be acknowledged.</strong></li>
<li><strong>Assess whether claims have been appropriately bounded.</strong> Empirical improvements may be sufficient, but if the interpretation still overreaches relative to the design, this should be flagged even if the data analysis itself has improved.</li>
</ul>
<hr />
<h2>8. Rhetorical Strategies</h2>
<h3>Hypothetical scenarios to test logic:</h3>
<ul>
<li>"Consider a scenario where the true variability in [X] is zero... Would these models return the correct answer?"</li>
</ul>
<h3>Constructive redirection:</h3>
<p>When a paper's approach is fundamentally flawed, suggest an alternative:
- "A different path to take, which I would find more compelling, would be to..."
- "I think you should focus on one thing... Focusing exclusively on this analysis frees up substantial space to devote adequate time to addressing the other issues."</p>
<h3>Building intuition before formal critique:</h3>
<ul>
<li>"I think it would be very helpful to build the credibility of the analysis by first presenting descriptive evidence that you can ultimately formalize with the model estimates."</li>
</ul>
<hr />
<h2>9. Substantive Domain Expertise</h2>
<p>Based on the corpus of reviews, the areas of deepest expertise (where the most technically specific critiques are offered) include:</p>
<ul>
<li>Teacher and principal labor markets (turnover, recruitment, retention, mobility)</li>
<li>Value-added modeling (for teachers, principals, coaches, schools)</li>
<li>Causal inference in education (DiD, FE, IV, RDD approaches)</li>
<li>School leadership and principal effectiveness</li>
<li>Teacher preparation and licensure</li>
<li>Education policy evaluation (turnaround, accountability, school choice)</li>
<li>Quantitative methods for large-scale administrative data</li>
<li>Race and representation in education workforce</li>
</ul>
<hr />
<h2>10. Review Workflow (Step-by-Step)</h2>
<ol>
<li><strong>Read the paper once through</strong> without taking notes, to get the overall arc.</li>
<li><strong>Read again</strong> with attention to the identification strategy, data, and core results.</li>
<li><strong>Draft the opening:</strong> Summarize the paper factually in 2-3 sentences (data, question, method, findings).</li>
<li><strong>Formulate the overall assessment:</strong> Is this paper answering its question credibly? Is the contribution clear?</li>
<li><strong>Identify 3-6 major issues</strong> and organize them thematically. Write each as a developed paragraph.</li>
<li><strong>List minor issues</strong> — presentation, writing, small analytical suggestions.</li>
<li><strong>Write the closing</strong> that matches the assessment.</li>
<li><strong>Review for tone:</strong> Ensure directness without hostility. Check that praise is genuine and critique is constructive.</li>
</ol>
<hr />
<h2>11. Things to Avoid</h2>
<ul>
<li><strong>Don't be vague.</strong> Every critique should be specific enough that the authors know exactly what to fix.</li>
<li><strong>Don't manufacture positives</strong> to soften a negative review. If the paper has fundamental problems, say so clearly.</li>
<li><strong>Don't raise issues you can't articulate clearly.</strong> If a concern is half-formed, develop it further or leave it out.</li>
<li><strong>Don't focus on minor issues at the expense of major ones.</strong> The review should prioritize what matters most for the paper's contribution.</li>
<li><strong>Don't demand a different paper.</strong> Review the paper that was submitted, not the one you would have written. (Exception: when the design fundamentally cannot answer the stated question, redirection is appropriate.)</li>
<li><strong>Don't be dismissive of qualitative or descriptive work.</strong> Apply the same constructive rigor, adapted to the appropriate standards for that methodology.</li>
<li><strong>Don't use numbered page-by-page comments as the primary structure.</strong> Organize thematically so the major issues are clear.</li>
</ul>
</body>
</html>